# -*- coding: utf-8 -*-
"""bad_smells.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6nxYf2Ztm2R9ZUKJtAEwlpmIUe10wHA
"""

!pip install -U pip
!pip install -U setuptools wheel
!pip uninstall -y mkl
!pip install --upgrade mxnet-cu100
!pip install autogluon
!pip install -U ipykernel

from autogluon.tabular import TabularPredictor as task
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import files
from pandas import read_csv
from sklearn.preprocessing import LabelEncoder

"""## Load data set
In the following sections you can load the data set you are going to use to test Auto-Keras. The training data is contained in `x_train` and `y_train`, while the test data is in `x_test` and `y_test`.
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
data=pd.read_csv('allNewDataSet.csv')

# data=data.drop(columns=['AvgCyclomatic','AvgCyclomaticModified','AvgCyclomaticStrict','AvgEssential','AvgLine','AvgLineBlank','AvgLineCode','AvgLineComment','MaxCyclomatic','MaxCyclomaticModified','MaxCyclomaticStrict','MaxEssential','MaxInheritanceTree','MaxNesting'])

# split into inputs and outputs
from sklearn import preprocessing
lb=preprocessing.LabelEncoder()
lb.fit(data["FEStatus"])
print(lb.classes_)
data["FEStatus"]=lb.transform(data["FEStatus"])

lbKind=preprocessing.LabelEncoder()
lbKind.fit(data["Kind"])
print(lbKind.classes_)
data["Kind"]=lbKind.transform(data["Kind"])

from scipy import stats
import numpy as np
z = np.abs(stats.zscore(data))
threshold = 7
print(np.where(z > threshold))
data = data[(z < threshold).all(axis=1)]
data.shape

columns=data.columns
data=data.dropna()

...
# summarize the class distribution
from collections import Counter
target = data.values[:,-1]
counter = Counter(target)
for k,v in counter.items():
	per = v / len(target) * 100
	print('Class=%s, Count=%d, Percentage=%.3f%%' % (lb.classes_[int(k)], v, per))

# evaluate a model
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
def evaluate_model(X, y, model):
	# define evaluation procedure
	cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
	# evaluate model
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores

label='FEStatus'
#get independant features and dependant variable
X = data.drop([label],axis=1).values   # independant features
y = data[label].values					# dependant variable

from sklearn.dummy import DummyClassifier
from numpy import mean
from numpy import std
model = DummyClassifier(strategy='most_frequent')
# evaluate the model
scores = evaluate_model(X, y, model)
# summarize performance
print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

counter = Counter(y)
for k,v in counter.items():
	per = v / len(y) * 100
	print('Class=%s, Count=%d, Percentage=%.3f%%' % (lb.classes_[int(k)], v, per))

train, validate, test = np.split(data.sample(frac=1, random_state=42),
                       [int(.80*len(data)), int(.90*len(data))])

counter = Counter(train[label].values	)
for k,v in counter.items():
	per = v / len(train[label]) * 100
	print('Class=%s, Count=%d, Percentage=%.3f%%' % (lb.classes_[int(k)], v, per))

#   oversampling a multi-class classification dataset
from pandas import read_csv
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN
from collections import Counter
from matplotlib import pyplot
from sklearn.preprocessing import LabelEncoder
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
from sklearn.preprocessing import OneHotEncoder
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import TomekLinks
from imblearn.pipeline import Pipeline
data2 = train.values
# split into input and output elements
X, y = data2[:, :-1], data2[:, -1]
# transform the dataset
oversample = SMOTETomek(tomek=TomekLinks())
X, y = oversample.fit_resample(X, y)
rus = RandomUnderSampler(sampling_strategy={0:2000,1:2000,2:2000,3:2000})
# X, y=rus.fit_resample(X, y)
# summarize distribution
counter = Counter(y)
for k,v in counter.items():
	per = v / len(y) * 100
	print('Class=%s, n=%d (%.3f%%)' % (lb.classes_[int(k)-1], v, per))
# plot the distribution
pyplot.bar(counter.keys(), counter.values())
pyplot.show()

#reLabling data
columns2=train.columns
columns2=columns2.delete(44)
X_farme=pd.DataFrame(X,columns=columns2)
train2=pd.concat([X_farme, pd.DataFrame(y)], axis=1, ignore_index=True)
#rename Columns
train2.columns=train.columns

train2

stats.ks_2samp(test.CountDeclClassMethod, validate.CountDeclClassMethod)

"""## Create  model

"""

# from sklearn.utils import class_weight
# class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)
# print(np.unique(y),class_weights)

import autogluon.core as ag

train2

ag.metrics.log_loss

# specify your evaluation metric here metric ='balanced_accuracy_score'
save_path = 'agModels-predictClass2'  # specifies folder to store trained models
metric = 'log_loss'
#
# time_limit='auto'

time_limit = 60*60  # train various models for ~2 min
num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model
search_strategy = 'auto'  # to tune hyperparameters using Bayesian optimization routine with a local scheduler

hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified
    'num_trials': num_trials,
    'searcher': search_strategy,
    'output_directory':save_path,
      'scheduler' : 'local'
}
# ,
#  hyperparameters=hyperparameters,time_limit=time_limit,

predictor = task(label=label, eval_metric=metric,path=save_path).fit(
    train2, tuning_data=validate, time_limit =time_limit,
    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs
)

predictor = task.load(save_path)

train_data_nolabel=train2.drop([label], axis=1)
y_train=train2[label];
y_pred = predictor.predict_proba(train_data_nolabel)
print("Predictions:  ", list(y_pred)[:50])
perf = predictor.evaluate_predictions(y_true=y_train, y_pred=y_pred, auxiliary_metrics=True)

validate_data_nolabel=validate.drop([label], axis=1)
y_validate=validate[label];
y_pred = predictor.predict_proba(validate_data_nolabel)
print("Predictions:  ", list(y_pred)[:50])
perf = predictor.evaluate_predictions(y_true=y_validate, y_pred=y_pred, auxiliary_metrics=True)

test_data_nolabel=test.drop([label], axis=1)
y_test=test[label];
y_pred = predictor.predict_proba (test_data_nolabel)
print("Predictions:  ", list(y_pred)[:50])
perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)

y_pred.head(50)

y_test.head(50)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
"""
Use your classification model to predict some labels
Then, plot confusion matrix and classification report using below code
y_test: real labels
y_pred: predicted model labels
"""
labels = [0,1,2,3]
y_pred2 = predictor.predict (test_data_nolabel)
print(classification_report(y_test, y_pred2)) #classification report from sklearn
cnf_matrix = confusion_matrix(y_test, y_pred2)
plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid
threshold = cnf_matrix.max() / 2 #threshold to define text color
for i in range(cnf_matrix.shape[0]): #print text in grid
    for j in range(cnf_matrix.shape[1]):
        plt.text(j, i, cnf_matrix[i,j], color="w" if cnf_matrix[i,j] > threshold else 'black')
tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes
plt.xticks(tick_marks, labels, rotation=45)
plt.yticks(tick_marks, labels)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.colorbar()
plt.tight_layout()

results = predictor.fit_summary()

# get the best performing model
model = predictor.get_model_best()

specific_model = predictor._trainer.load_model(model)
model_info=specific_model.get_info
print(model_info)

# summarize the loaded model
predictor.leaderboard(test, silent=True).sort_values(by='score_test', ascending=False)

all_models = predictor.get_model_names()
model_to_use = all_models[0]
specific_model = predictor._trainer.load_model(model_to_use)

# Objects defined below are dicts of various information (not printed here as they are quite large):
model_info = specific_model.get_info()
predictor_information = predictor.info()

predictor_information

feature_importance=predictor.feature_importance(test,subsample_size=1000)

feature_importance.head(20)

# Save and download model
import os
print(os.listdir("."))

"""### Export model
You can export the model for later training.
"""

import zipfile

def zip(src, dst):
    zf = zipfile.ZipFile("%s.zip" % (dst), "w", zipfile.ZIP_DEFLATED)
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            absname = os.path.abspath(os.path.join(dirname, filename))
            arcname = absname[len(abs_src) + 1:]
            print ('zipping %s as %s' % (os.path.join(dirname, filename),
                                        arcname))
            zf.write(absname, arcname)
    zf.close()

zip(save_path, "./dst")

files.download("./dst.zip")

"""### **testing**

"""

uploaded2 = files.upload()
new_system_data=pd.read_csv('freecollall.csv')
new_system_data["Kind"]=lbKind.transform(new_system_data["Kind"])
new_system_y_pred = predictor.predict(new_system_data)
new_system_data["FEStatus"]=new_system_y_pred
new_system_data.to_csv("freecellPredicted.csv", index=False)
files.download("freecellPredicted.csv")

uploaded3 = files.upload()
new_system_data=pd.read_csv('freecolِAfterRefactoring.csv')
new_system_data["Kind"]=lbKind.transform(new_system_data["Kind"])
new_system_y_pred = predictor.predict(new_system_data)
new_system_data["FEStatus"]=new_system_y_pred
new_system_data.to_csv("freecolِAfterRefactoringPredicted.csv", index=False)
files.download("freecolِAfterRefactoringPredicted.csv")

"""### Load model
Note that when loading a model, the class of `clf` changes to `PortableImageSupervised`.

Subsequently use [this website](https://lutzroeder.github.io/netron/) to visualize the model.
"""